import com.typesafe.config.Config
import org.apache.spark.sql.SparkSession

object GCSConnectionHandler extends BaseJob {

  override def executeJob(config: Config, spark: SparkSession): Unit = {
    val readPath = "/home/mt24070/Downloads/BigMart Sales.csv"
    val writePath = config.getString("hadoopOptions.fs.defaultsFs") + "/output/BigMart_Sales"

    // Read Data from Local Storage
    val inputDF = spark.read
      .option("inferSchema", "true")
      .option("header", "true")
      .option("delimiter", ";")
      .csv(readPath)

    inputDF.show(truncate = false)

    // Write Data to GCS Storage
    inputDF.write
      .option("header", "true")
      .mode("overwrite")
      .csv(writePath)
  }
}


